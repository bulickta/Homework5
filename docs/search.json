[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "ST 558 Homework5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\n\nCross-validation can first help with model selection for a smaller data set where separating into training and test sets might not leave enough training data for effective modelling. Alternatively, Cross-validation can help to select the best value for the tuning parameter (value of m) of the random forest.\n\n\nDescribe the bagged tree algorithm.\n\n\nThe bagged tree algorithm essentially involves repeatedly sampling (with replacement) from the original sample, training a tree on the new sample and calculating predictions, and then averaging each prediction across the number of iterations of the algorithm, so ultimately there is a mean value for each prediction. Alternatively, in a classification version the same process is completed, but the “mean” value is instead just selected to be the most common result for each prediction.\n\n\nWhat is meant by a general linear model?\n\n\nA general linear model is a model that can take in both continuous and categorical data as predictors, and expects a continuous reponse variable with normally distributed residuals. This is in contrast to a “generalized” linear model with more flexible options for the reponse and flexibility in the expected distribution of the residuals.\n\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\n\nAn interaction term allows the model to account for the possibility that the relationship between a certain predictor and the response may be influenced by a relationship between the predictor and another predictor. For instance, if there is a relationship between the weight of a vehicle and its expected miles per gallon, this relationship might also be changed by the number of cyclinders in the vehicle’s engine, separate from the direct relationship between cylinders and MPG. An interaction term allows the model to account for this additional complexity.\n\n\nWhy do we split our data into a training and test set?\n\n\nSplitting data into a training set and a test set is what allows for the comparison of prediction efficacy across different models, as several models can be trained on the same training set, and then a metric for model error, such as RMSE, can be compared to see which is “better” at prediction."
  },
  {
    "objectID": "Homework5.html#task-1-conceptual-questions",
    "href": "Homework5.html#task-1-conceptual-questions",
    "title": "ST 558 Homework5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\n\nCross-validation can first help with model selection for a smaller data set where separating into training and test sets might not leave enough training data for effective modelling. Alternatively, Cross-validation can help to select the best value for the tuning parameter (value of m) of the random forest.\n\n\nDescribe the bagged tree algorithm.\n\n\nThe bagged tree algorithm essentially involves repeatedly sampling (with replacement) from the original sample, training a tree on the new sample and calculating predictions, and then averaging each prediction across the number of iterations of the algorithm, so ultimately there is a mean value for each prediction. Alternatively, in a classification version the same process is completed, but the “mean” value is instead just selected to be the most common result for each prediction.\n\n\nWhat is meant by a general linear model?\n\n\nA general linear model is a model that can take in both continuous and categorical data as predictors, and expects a continuous reponse variable with normally distributed residuals. This is in contrast to a “generalized” linear model with more flexible options for the reponse and flexibility in the expected distribution of the residuals.\n\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\n\nAn interaction term allows the model to account for the possibility that the relationship between a certain predictor and the response may be influenced by a relationship between the predictor and another predictor. For instance, if there is a relationship between the weight of a vehicle and its expected miles per gallon, this relationship might also be changed by the number of cyclinders in the vehicle’s engine, separate from the direct relationship between cylinders and MPG. An interaction term allows the model to account for this additional complexity.\n\n\nWhy do we split our data into a training and test set?\n\n\nSplitting data into a training set and a test set is what allows for the comparison of prediction efficacy across different models, as several models can be trained on the same training set, and then a metric for model error, such as RMSE, can be compared to see which is “better” at prediction."
  },
  {
    "objectID": "Homework5.html#task-2-fitting-models",
    "href": "Homework5.html#task-2-fitting-models",
    "title": "ST 558 Homework5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\n\nEDA and Data Preparation\nWe begin by reading in our data of interest, and checking for any missing values. Notably, there is no “missing” data, but as we will see in the plots below, there are a few data issues, especially with Cholesterol containing a significant number of “0” values, which is obviously not a real measurement. That said, none of the variables have issues that would ultimately prevent them from usage.\n\nrawdata &lt;- read_csv(\"./heart.csv\")\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmissing1 &lt;- colSums(is.na(rawdata))\nmissing1\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\nNext, we then create some summaries, noting from the summary statistics that the wide variety of scales indicates we should like center and scale all our data (which is included in all models in this assignment), and noting that several variables appear to correlate significantly with Heart Disease, namely: Exercise Angina, Age, Max HR, and Oldpeak.\n\nsummary(rawdata)\n\n      Age            Sex            ChestPainType        RestingBP    \n Min.   :28.00   Length:918         Length:918         Min.   :  0.0  \n 1st Qu.:47.00   Class :character   Class :character   1st Qu.:120.0  \n Median :54.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :53.51                                         Mean   :132.4  \n 3rd Qu.:60.00                                         3rd Qu.:140.0  \n Max.   :77.00                                         Max.   :200.0  \n  Cholesterol      FastingBS       RestingECG            MaxHR      \n Min.   :  0.0   Min.   :0.0000   Length:918         Min.   : 60.0  \n 1st Qu.:173.2   1st Qu.:0.0000   Class :character   1st Qu.:120.0  \n Median :223.0   Median :0.0000   Mode  :character   Median :138.0  \n Mean   :198.8   Mean   :0.2331                      Mean   :136.8  \n 3rd Qu.:267.0   3rd Qu.:0.0000                      3rd Qu.:156.0  \n Max.   :603.0   Max.   :1.0000                      Max.   :202.0  \n ExerciseAngina        Oldpeak          ST_Slope          HeartDisease   \n Length:918         Min.   :-2.6000   Length:918         Min.   :0.0000  \n Class :character   1st Qu.: 0.0000   Class :character   1st Qu.:0.0000  \n Mode  :character   Median : 0.6000   Mode  :character   Median :1.0000  \n                    Mean   : 0.8874                      Mean   :0.5534  \n                    3rd Qu.: 1.5000                      3rd Qu.:1.0000  \n                    Max.   : 6.2000                      Max.   :1.0000  \n\ngg&lt;-ggplot(rawdata,aes(x=as_factor(HeartDisease)))\na &lt;- gg+geom_bar()+labs(x=\"Heart Disease\")\nb &lt;- gg+geom_count(aes(y=Sex))+labs(x=\"Heart Disease\")\nc &lt;- gg+geom_count(aes(y=ChestPainType))+labs(x=\"Heart Disease\")\nd &lt;- gg+geom_count(aes(y=FastingBS))+labs(x=\"Heart Disease\")\ne &lt;- gg+geom_count(aes(y=RestingECG))+labs(x=\"Heart Disease\")\nf &lt;- gg+geom_count(aes(y=ExerciseAngina))+labs(x=\"Heart Disease\")\ng &lt;- gg+geom_boxplot(aes(y=Age,fill=as_factor(HeartDisease)))+labs(x=\"Heart Disease\")\nh &lt;- gg+geom_boxplot(aes(y=RestingBP,fill=as_factor(HeartDisease)))+labs(x=\"Heart Disease\")\ni &lt;- gg+geom_boxplot(aes(y=Cholesterol,fill=as_factor(HeartDisease)))+labs(x=\"Heart Disease\")\nj &lt;- gg+geom_boxplot(aes(y=MaxHR,fill=as_factor(HeartDisease)))+labs(x=\"Heart Disease\")\nk &lt;- gg+geom_boxplot(aes(y=Oldpeak,fill=as_factor(HeartDisease)))+labs(x=\"Heart Disease\")\n\nggarrange(a,b,c,d,e,f,g,h,i,j,k,ncol=2,nrow=2)\n\n$`1`\n\n\n\n\n\n\n\n\n\n\n$`2`\n\n\n\n\n\n\n\n\n\n\n$`3`\n\n\n\n\n\n\n\n\n\n\nattr(,\"class\")\n[1] \"list\"      \"ggarrange\"\n\n\n\n\nCleaning and Splitting Data\nAfter our EDA, we do some data transformations, ultimately outputting four data sets: “training” and “test,” which are partitions of the original data containing dummy variables for the four categorical variables (needed for for the KNN model), and “training2” and “test2” which has the same partition but without needing the extra dummy variables for ease of use in the other models.\n\nselectdata &lt;- rawdata |&gt;\n  select(Age:Oldpeak,HeartDisease)\n\nDummies &lt;- predict(dummyVars(HeartDisease~.,data=selectdata),newdata=selectdata) |&gt;\n  as_tibble()|&gt;\n  select(starts_with(\"Sex\"),starts_with(\"Chest\"),starts_with(\"Resting\"),starts_with(\"Exer\")) |&gt;\n  select(SexF:ChestPainTypeTA,RestingECGLVH:ExerciseAnginaY)\n\ncleandata &lt;- bind_cols(selectdata,Dummies) |&gt;\n  mutate(HeartDisease = as_factor(HeartDisease)) |&gt;\n  select(Age,RestingBP:FastingBS,MaxHR,Oldpeak:ExerciseAnginaY)\n\ncleandata2 &lt;- selectdata |&gt;\n  mutate(HeartDisease = as_factor(HeartDisease))\n\nindex &lt;- createDataPartition(cleandata$HeartDisease,p = 0.7,list=FALSE)\ntraining &lt;- cleandata[index,]\ntest &lt;- cleandata[-index,]\ntraining2 &lt;- cleandata2[index,]\ntest2 &lt;- cleandata2[-index,]\n\nhead(training)\n\n# A tibble: 6 × 18\n    Age RestingBP Cholesterol FastingBS MaxHR Oldpeak HeartDisease  SexF  SexM\n  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1    49       160         180         0   156     1   1                1     0\n2    37       130         283         0    98     0   0                0     1\n3    54       150         195         0   122     0   0                0     1\n4    45       130         237         0   170     0   0                1     0\n5    37       140         207         0   130     1.5 1                0     1\n6    48       120         284         0   120     0   0                1     0\n# ℹ 9 more variables: ChestPainTypeASY &lt;dbl&gt;, ChestPainTypeATA &lt;dbl&gt;,\n#   ChestPainTypeNAP &lt;dbl&gt;, ChestPainTypeTA &lt;dbl&gt;, RestingECGLVH &lt;dbl&gt;,\n#   RestingECGNormal &lt;dbl&gt;, RestingECGST &lt;dbl&gt;, ExerciseAnginaN &lt;dbl&gt;,\n#   ExerciseAnginaY &lt;dbl&gt;\n\nhead(test)\n\n# A tibble: 6 × 18\n    Age RestingBP Cholesterol FastingBS MaxHR Oldpeak HeartDisease  SexF  SexM\n  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1    40       140         289         0   172     0   0                0     1\n2    48       138         214         0   108     1.5 1                1     0\n3    39       120         339         0   170     0   0                0     1\n4    54       110         208         0   142     0   0                0     1\n5    42       115         211         0   137     0   0                1     0\n6    43       120         201         0   165     0   0                1     0\n# ℹ 9 more variables: ChestPainTypeASY &lt;dbl&gt;, ChestPainTypeATA &lt;dbl&gt;,\n#   ChestPainTypeNAP &lt;dbl&gt;, ChestPainTypeTA &lt;dbl&gt;, RestingECGLVH &lt;dbl&gt;,\n#   RestingECGNormal &lt;dbl&gt;, RestingECGST &lt;dbl&gt;, ExerciseAnginaN &lt;dbl&gt;,\n#   ExerciseAnginaY &lt;dbl&gt;\n\nhead(training2)\n\n# A tibble: 6 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    49 F     NAP                 160         180         0 Normal       156\n2    37 M     ATA                 130         283         0 ST            98\n3    54 M     NAP                 150         195         0 Normal       122\n4    45 F     ATA                 130         237         0 Normal       170\n5    37 M     ASY                 140         207         0 Normal       130\n6    48 F     ATA                 120         284         0 Normal       120\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\nhead(test2)\n\n# A tibble: 6 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    48 F     ASY                 138         214         0 Normal       108\n3    39 M     NAP                 120         339         0 Normal       170\n4    54 M     ATA                 110         208         0 Normal       142\n5    42 F     NAP                 115         211         0 ST           137\n6    43 F     ATA                 120         201         0 Normal       165\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\n\n\nkNN Model\nWe fit the kNN model, noting the optimal parameter of k below, and when predicting test data resulting in an accuracy of 80.73%. Notably, I chose to fit the full model with all explanatory variables.\n\nknnModel &lt;- train(HeartDisease~.,data=training,method=\"knn\",\n                  trControl=trainControl(method=\"repeatedcv\",number=10,repeats = 3),\n                  preProcess=c(\"center\",\"scale\"),tuneGrid=expand.grid(k=c(1:40)))\n\nknnModel$bestTune\n\n    k\n24 24\n\nconfusionMatrix(data=test$HeartDisease,reference=predict(knnModel,newdata=test))$overall[1]\n\n Accuracy \n0.8072727 \n\n\n\n\nLogistic Regression Models\nNext we fit three logistic regression models - one containing the full set of all potential explanatory variables, one only using Age as a variable, and one using the four variables that appear to correlate most strongly: exercise angina, age, oldpeak, and max HR.\n\nlogModel1 &lt;- train(HeartDisease~.,data=training2,method=\"glm\",family=\"binomial\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats = 3))\n\nlogModel2 &lt;- train(HeartDisease~Age,data=training2,method=\"glm\",family=\"binomial\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats = 3))\n\nlogModel3 &lt;- train(HeartDisease~ExerciseAngina*Age*Oldpeak*MaxHR,data=training2,method=\"glm\",family=\"binomial\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats = 3))\n\nComparing these three models, we see the preferred model according to AIC is the full model, and according to the test prediction accuracy is also the full model, so of these three options we select the full logistic regression model.\n\nlogModel1$finalModel\n\n\nCall:  NULL\n\nCoefficients:\n     (Intercept)               Age              SexM  ChestPainTypeATA  \n          0.3797            0.1202            0.4791           -0.8441  \nChestPainTypeNAP   ChestPainTypeTA         RestingBP       Cholesterol  \n         -0.5629           -0.3294            0.1410           -0.3136  \n       FastingBS  RestingECGNormal      RestingECGST             MaxHR  \n          0.5068           -0.1138           -0.1196           -0.4199  \n ExerciseAnginaY           Oldpeak  \n          0.6090            0.6321  \n\nDegrees of Freedom: 642 Total (i.e. Null);  629 Residual\nNull Deviance:      884 \nResidual Deviance: 498.4    AIC: 526.4\n\nlogModel2$finalModel\n\n\nCall:  NULL\n\nCoefficients:\n(Intercept)          Age  \n     0.2286       0.5671  \n\nDegrees of Freedom: 642 Total (i.e. Null);  641 Residual\nNull Deviance:      884 \nResidual Deviance: 837.3    AIC: 841.3\n\nlogModel3$finalModel\n\n\nCall:  NULL\n\nCoefficients:\n                        (Intercept)                      ExerciseAnginaY  \n                           0.453333                            13.376594  \n                                Age                              Oldpeak  \n                           0.752420                             5.771061  \n                              MaxHR                `ExerciseAnginaY:Age`  \n                          -0.003251                           -10.371745  \n          `ExerciseAnginaY:Oldpeak`                        `Age:Oldpeak`  \n                         -13.189308                            -4.440046  \n            `ExerciseAnginaY:MaxHR`                          `Age:MaxHR`  \n                         -10.354868                            -0.527107  \n                    `Oldpeak:MaxHR`        `ExerciseAnginaY:Age:Oldpeak`  \n                          -4.270088                            11.233205  \n        `ExerciseAnginaY:Age:MaxHR`      `ExerciseAnginaY:Oldpeak:MaxHR`  \n                           8.091418                            10.218443  \n                `Age:Oldpeak:MaxHR`  `ExerciseAnginaY:Age:Oldpeak:MaxHR`  \n                           3.537924                            -8.225794  \n\nDegrees of Freedom: 642 Total (i.e. Null);  627 Residual\nNull Deviance:      884 \nResidual Deviance: 618.2    AIC: 650.2\n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(logModel1,newdata=test2))$overall[1]\n\n Accuracy \n0.8218182 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(logModel2,newdata=test2))$overall[1]\n\n Accuracy \n0.6327273 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(logModel3,newdata=test2))$overall[1]\n\n Accuracy \n0.7890909 \n\n\n\n\nTree Models\nFor the classification tree model, random forest model, and boosted tree model, I tried three combinations of variables for each, mimicking the options for logistic regression with a full model, an age-only model, and an Angina/Age/Oldpeak/Max HR model\n\ntreeModel1 &lt;- train(HeartDisease~.,data=training2,method=\"rpart\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(cp=seq(0,0.1,0.001)))\n\ntreeModel2 &lt;- train(HeartDisease~Age,data=training2,method=\"rpart\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(cp=seq(0,0.1,0.001)))\n\ntreeModel3 &lt;- train(HeartDisease~ExerciseAngina*Age*Oldpeak*MaxHR,data=training2,method=\"rpart\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(cp=seq(0,0.1,0.001)))\n\nforestModel1 &lt;- train(HeartDisease~.,data=training2,method=\"rf\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(mtry=c(1:10)))\n\nforestModel2 &lt;- train(HeartDisease~Age,data=training2,method=\"rf\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(mtry=c(1)))\n\nforestModel3 &lt;- train(HeartDisease~ExerciseAngina*Age*Oldpeak*MaxHR,data=training2,method=\"rf\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(mtry=c(1:4)))\n\nboostModel1 &lt;- train(HeartDisease~.,data=training2,method=\"gbm\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(n.trees=c(25,50,100,200),interaction.depth=c(1,2,3),shrinkage=0.1,n.minobsinnode=10),verbose = FALSE)\n\nboostModel2 &lt;- train(HeartDisease~Age,data=training2,method=\"gbm\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(n.trees=c(25,50,100,200),interaction.depth=c(1,2,3),shrinkage=0.1,n.minobsinnode=10),verbose = FALSE)\n\nboostModel3 &lt;- train(HeartDisease~ExerciseAngina*Age*Oldpeak*MaxHR,data=training2,method=\"gbm\",preProcess=c(\"center\",\"scale\"),trControl=trainControl(method=\"repeatedcv\",number=10,repeats=3),tuneGrid=expand.grid(n.trees=c(25,50,100,200),interaction.depth=c(1,2,3),shrinkage=0.1,n.minobsinnode=10),verbose = FALSE)\n\nComparing the tree models, we see that according to the prediction accuracy the full model again should be selected, with an optimal tuning of cp noted below.\n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(treeModel1,newdata=test2))$overall[1]\n\n Accuracy \n0.8181818 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(treeModel2,newdata=test2))$overall[1]\n\n Accuracy \n0.6727273 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(treeModel3,newdata=test2))$overall[1]\n\n Accuracy \n0.7890909 \n\ntreeModel1$best\n\n      cp\n16 0.015\n\n\nComparing the forest models, we see that according to the prediction accuracy the full variable model again should be selected, with optimal tuning of mtry noted below.\n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(forestModel1,newdata=test2))$overall[1]\n\n Accuracy \n0.8472727 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(forestModel2,newdata=test2))$overall[1]\n\nAccuracy \n    0.64 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(forestModel3,newdata=test2))$overall[1]\n\n Accuracy \n0.7709091 \n\nforestModel1$bestTune\n\n  mtry\n1    1\n\n\nComparing the boosted models, we see that according to prediction accuracy, the full variable model again should be selected, with optimal tuning variables noted below\n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(boostModel1,newdata=test2))$overall[1]\n\n Accuracy \n0.8581818 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(boostModel2,newdata=test2))$overall[1]\n\n Accuracy \n0.6327273 \n\nconfusionMatrix(reference=test2$HeartDisease,data=predict(boostModel3,newdata=test2))$overall[1]\n\n Accuracy \n0.7927273 \n\nboostModel1$bestTune\n\n  n.trees interaction.depth shrinkage n.minobsinnode\n7     100                 2       0.1             10\n\n\nLastly, comparing all of our models together, we see that the ultimate winner for best prediction of this data is the Boosted Tree model using all possible explanatory variables, and with tuning variables as noted below. Although it should be noted that with the calculated confidence intervals, most of these results do technically overlap, suggesting it could be preferable to instead choose the least resource intensive model.\n\nknnResult &lt;- confusionMatrix(data=test$HeartDisease,reference=predict(knnModel,newdata=test))$overall[c(1,3,4)]\nlogResult &lt;- confusionMatrix(reference=test2$HeartDisease,data=predict(logModel1,newdata=test2))$overall[c(1,3,4)]\ntreeResult &lt;- confusionMatrix(reference=test2$HeartDisease,data=predict(treeModel1,newdata=test2))$overall[c(1,3,4)]\nforestResult &lt;- confusionMatrix(reference=test2$HeartDisease,data=predict(forestModel1,newdata=test2))$overall[c(1,3,4)]\nboostResult &lt;- confusionMatrix(reference=test2$HeartDisease,data=predict(boostModel1,newdata=test2))$overall[c(1,3,4)]\nresults &lt;-  as.data.frame(bind_cols(knnResult,logResult,treeResult,forestResult,boostResult))\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n\ncolnames(results) &lt;- c(\"kNN\",\"Logistic Regression\",\"Classification Tree\",\"Random Forest\",\"Boosted Tree\")\nrownames(results) &lt;- c(\"Accuracy (Percentage)\",\"Lower CI\",\"Upper CI\")\nresults\n\n                            kNN Logistic Regression Classification Tree\nAccuracy (Percentage) 0.8218182           0.8218182           0.8181818\nLower CI              0.7713710           0.7713710           0.7674219\nUpper CI              0.8651766           0.8651766           0.8619357\n                      Random Forest Boosted Tree\nAccuracy (Percentage)     0.8472727    0.8581818\nLower CI                  0.7992153    0.8112668\nUpper CI                  0.8876574    0.8971707\n\nboostModel1$bestTune\n\n  n.trees interaction.depth shrinkage n.minobsinnode\n7     100                 2       0.1             10"
  }
]